#深度学习 

## YOLO 是什么？

YOLO是目标检测模型。

目标检测是计算机视觉中比较简单的任务，用来在一张图篇中找到**某些特定的物体**，目标检测不仅要求我们识别这些物体的**种类**，同时要求我们标出这些物体的**位置**。

显然，类别是离散数据，位置是连续数据。

![](https://pic1.zhimg.com/v2-88ca6ec106e7647a0390ad3d93625444_b.jpg)

上面的图片中，分别是计算机视觉的三类任务：分类，目标检测，实例分割。

很显然，整体上这三类任务从易到难，我们要讨论的目标检测位于中间。前面的分类任务是我们做**目标检测的基础**，至于**像素级别**的实例分割，太难了别想了。

## YOLO 原理

在这之前，我们再重申一下我们的任务。我们的目的是在一张图片中找出物体，并给出它的类别和位置。目标检测是基于监督学习的，每张图片的监督信息是它所包含的N个物体，每个物体的信息有**五个**，分别是物体的**中心位置(x,y)**和它的**高(h)**和**宽(w)**，最后是它的类别。

YOLO 的预测是基于整个图片的，并且它会一次性输出所有检测到的目标信息，包括类别和位置。

就好像捕鱼一样，R-CNN是先选好哪里可能出现鱼，而YOLO是直接一个大网下去，把所有的鱼都捞出来。

先假设我们处理的图片是一个**正方形**。

YOLO的第一步是**分割图片**，它将图片分割为 S2 个grid，每个grid的大小都是相等的，像这样：

![](https://pic1.zhimg.com/v2-01c11f4d423a698271159fcc98524894_b.jpg)

如果我们让每个框只能识别出一个物体，且要求这个物体必须在这个框之内，那YOLO就变成了很蠢的滑窗法了。

YOLO的聪明之处在于，它只要求这个物体的**中心**落在这个框框之中。

这意味着，我们不用设计非常非常**大**的框，因为我们只需要让物体的中心在这个框中就可以了，而不是必须要让整个物体都在这个框中。

具体怎么实现呢？

我们要让这个 S2 个框每个都预测出**B 个 bounding box**，这个 bounding box 有5个量，分别是物体的**中心位置(x,y)**和它的**高(h)**和**宽(w)**，以及这次预测的**置信度**。

每个框框不仅只预测B个bounding box，它还要负责预测这个框框中的物体**是什么类别**的，这里的类别用one-hot编码表示。

注意，虽然一个框框有多个bounding boxes，但是只能识别出一个物体，因此每个框框需要预测物体的类别，**而bounding box不需要**。

也就是说，如果我们有 S2 个框框，每个框框的 bounding boxes 个数为 B，分类器可以识别出 C 种不同的物体，那么所有整个 ground truth 的长度为：


先看这些bounding box显示出来是什么样的：

![](https://pic3.zhimg.com/v2-019dcd8dae979fa96f48fc23e70e25a2_b.jpg)

在上面的例子中，图片被分成了49个框，每个框预测2个bounding box，因此上面的图中有98个bounding box。

可以看到大致上每个框里确实有两个bounding box。

可以看到这些BOX中有的边框比较粗，有的比较细，这是**置信度**不同的表现，置信度高的比较粗，置信度低的比较细。
##  IOU指标
